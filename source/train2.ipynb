{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a040eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualMHSA(nn.Module):\n",
    "    def __init__(self, d_model, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(d_model, n_head, dropout=dropout, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        attn_mask = torch.triu(torch.ones(T, T, device=x.device) * float('-inf'), diagonal=1)\n",
    "        x_out, _ = self.mha(x, x, x, attn_mask=attn_mask)\n",
    "        return x_out\n",
    "\n",
    "class ConformerConvModule(nn.Module):\n",
    "    def __init__(self, d_model, kernel_size=15, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pointwise_conv1 = nn.Conv1d(d_model, d_model * 2, kernel_size=1)\n",
    "        self.glu = nn.GLU(dim=1)\n",
    "        self.depthwise_conv = nn.Conv1d(d_model, d_model, kernel_size, padding=(kernel_size-1)//2, groups=d_model)\n",
    "        self.batch_norm = nn.GroupNorm(num_groups=1, num_channels=d_model)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.pointwise_conv2 = nn.Conv1d(d_model, d_model, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.dropout(self.pointwise_conv2(self.activation(self.batch_norm(self.depthwise_conv(self.glu(self.pointwise_conv1(x)))))))\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "class FeedForwardModule(nn.Module):\n",
    "    def __init__(self, d_model, expansion_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(d_model, d_model*expansion_factor)\n",
    "        self.activation = nn.SiLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer2 = nn.Linear(d_model*expansion_factor, d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.layer2(self.dropout1(self.activation(self.layer1(x)))))\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, kernel_size=15, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ffn1 = FeedForwardModule(d_model, dropout=dropout)\n",
    "        self.conv_module = ConformerConvModule(d_model, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.self_attn = CasualMHSA(d_model, n_head, dropout=dropout)\n",
    "        self.ffn2 = FeedForwardModule(d_model, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model); self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model); self.norm4 = nn.LayerNorm(d_model)\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        x = x + 0.5 * self.ffn1(self.norm1(x))\n",
    "        x = x + self.conv_module(self.norm2(x))\n",
    "        x = x + self.self_attn(self.norm3(x))\n",
    "        x = x + 0.5 * self.ffn2(self.norm4(x))\n",
    "        return self.final_norm(x)\n",
    "\n",
    "class AECModel(nn.Module):\n",
    "    def __init__(self, d_model=128, n_fft=512, n_head=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.n_freq = n_fft // 2 + 1\n",
    "        input_dim = self.n_freq * 4 \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.layers = nn.ModuleList([ConformerBlock(d_model, n_head) for _ in range(num_layers)])\n",
    "        self.mask_proj = nn.Linear(d_model, self.n_freq * 2)\n",
    "\n",
    "    def forward(self, mic_stft, ref_stft):\n",
    "        B, F, T, C = mic_stft.shape\n",
    "        mic_flat = mic_stft.permute(0, 2, 1, 3).reshape(B, T, F * 2)\n",
    "        ref_flat = ref_stft.permute(0, 2, 1, 3).reshape(B, T, F * 2)\n",
    "        x = torch.cat([mic_flat, ref_flat], dim=2)\n",
    "        x = self.input_proj(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        mask = self.mask_proj(x).view(B, T, F, 2).permute(0, 2, 1, 3)\n",
    "        mic_real, mic_imag = mic_stft[..., 0], mic_stft[..., 1]\n",
    "        mask_real, mask_imag = mask[..., 0], mask[..., 1]\n",
    "        est_real = mic_real * mask_real - mic_imag * mask_imag\n",
    "        est_imag = mic_real * mask_imag + mic_imag * mask_real\n",
    "        return torch.stack([est_real, est_imag], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e558b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFT_Dataset():\n",
    "    def __init__(self, hf_dataset, n_fft=512, win_length=320, hop_length=160, duration=10):\n",
    "        self.dataset = hf_dataset\n",
    "        self.win_length, self.hoplength , self.n_fft = win_length, hop_length, n_fft\n",
    "        self.max_len = int(16000*duration)\n",
    "        self.window = torch.hann_window(self.win_length)  \n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def process_len(self, wav_nparray):\n",
    "        wav = torch.from_numpy(wav_nparray).float()\n",
    "        if wav.ndim == 1:\n",
    "            wav = wav.unsqueeze(0)\n",
    "        if wav.shape[1] > self.max_len:\n",
    "            start = random.randint(0, wav.shape[1]-self.max_len)\n",
    "            return wav[:,start:start + self.max_len], start\n",
    "        return torch.nn.functional.pad(wav, (0, self.max_len - wav.shape[1])), 0\n",
    "    \n",
    "    def get_stft(self, wav):\n",
    "        stft_complex = torch.stft(wav, n_fft=self.n_fft, hop_length=self.hop_length,\n",
    "                                  win_length=self.win_length, window=self.window,\n",
    "                                  center=True, return_complex=True)\n",
    "        return torch.view_as_real(stft_complex).squeeze(0)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Xử lý audio và lấy vị trí start để cắt VAD label tương ứng\n",
    "        mic_wav, start = self.process_len(item['mic']['array'])\n",
    "        ref_wav, _ = self.process_len(item['ref']['array'])\n",
    "        clean_wav, _ = self.process_len(item['clean']['array'])\n",
    "        \n",
    "        # Lấy nhãn VAD từ dataset (dạng List)\n",
    "        vad_full = torch.tensor(item['vad_label'], dtype=torch.long)\n",
    "        \n",
    "        # Cắt nhãn VAD khớp với đoạn audio (10ms mỗi nhãn)\n",
    "        num_frames = self.get_stft(mic_wav).shape[1] # Số khung T\n",
    "        start_frame = start // self.hop_length\n",
    "        vad_label = vad_full[start_frame : start_frame + num_frames]\n",
    "        \n",
    "        # Padding nhãn VAD nếu thiếu do làm tròn\n",
    "        if vad_label.shape[0] < num_frames:\n",
    "            vad_label = torch.nn.functional.pad(vad_label, (0, num_frames - vad_label.shape[0]))\n",
    "        elif vad_label.shape[0] > num_frames:\n",
    "            vad_label = vad_label[:num_frames]\n",
    "\n",
    "        return self.get_stft(mic_wav), self.get_stft(ref_wav), self.get_stft(clean_wav), vad_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vad_predict(vad_model, wav_est, sr=16000):\n",
    "    vad =\"\"\n",
    "    return vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_task_loss(est, target, vad_logits, vad_labels, beta=0.1):\n",
    "    \n",
    "    # 1. AEC Loss\n",
    "    mae = nn.L1Loss()\n",
    "    mag_est = torch.sqrt(est[..., 0]**2 + est[..., 1]**2 + 1e-9)\n",
    "    mag_target = torch.sqrt(target[..., 0]**2 + target[..., 1]**2 + 1e-9)\n",
    "    loss_aec = mae(mag_est, mag_target) + mae(est, target)\n",
    "\n",
    "    loss_vad = nn.CrossEntropyLoss()(vad_logits.transpose(1, 2), vad_labels)\n",
    "    \n",
    "    total_loss = loss_aec + beta * loss_vad\n",
    "    return total_loss, loss_aec, loss_vad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
